{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1939,"status":"ok","timestamp":1685722873314,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"1Htt3xUMWlut","outputId":"633ea7a7-2a95-446b-8b7b-9d9e4f985926"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFXCkdQ9qdus"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/SloganGenerator/venv/lib/python3.10/site-packages')"]},{"cell_type":"markdown","source":["#Load the model"],"metadata":{"id":"yZ4gDevx2Jzo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9tTd8E8WXEm"},"outputs":[],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","MODEL_NAME = 'distilgpt2'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1685722878353,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"JjAPK7wpYjg4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3cc73843-ac66-410b-ef9b-38eb258a4b61"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<context>', '<slogan>']}\n"]}],"source":["# Declare special tokens for padding and separating the context from the slogan:\n","SPECIAL_TOKENS_DICT = {\n","    'pad_token': '<pad>',\n","    'additional_special_tokens': ['<context>', '<slogan>'],\n","}\n","\n","# Add these special tokens to the vocabulary and resize model's embeddings:\n","tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Show the full list of special tokens:\n","print(tokenizer.special_tokens_map)"]},{"cell_type":"markdown","source":["#Create Dataset"],"metadata":{"id":"eQ1Li22j2VIu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3159,"status":"ok","timestamp":1685722884105,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"FKBEnqJsiN-Z","outputId":"abcf3c29-5978-4549-c12b-4e53cf316d58"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 64])\n","9520\n"]}],"source":["import csv\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","class SloganDataset(Dataset):\n","  def __init__(self, filename, tokenizer, seq_length=64):\n","\n","    context_tkn = tokenizer.additional_special_tokens_ids[0]\n","    slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n","    pad_tkn = tokenizer.pad_token_id\n","    eos_tkn = tokenizer.eos_token_id\n","\n","    self.examples = []\n","    with open(filename) as csvfile:\n","      reader = csv.reader(csvfile)\n","      for row in reader:\n","\n","        # Build the context and slogan segments:\n","        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\n","        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\n","        \n","        # Concatenate the two parts together adding <pad> token for the remaining lenght:\n","        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) )\n","\n","        # Annotate each token with its corresponding segment:\n","        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )\n","\n","        # Ignore the context, padding, and <slogan> tokens by setting their labels to -100\n","        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\n","\n","        # Add the preprocessed example to the dataset\n","        self.examples.append((tokens, segments, labels))\n","\n","  def __len__(self):\n","    return len(self.examples)\n","\n","  def __getitem__(self, item):\n","    \n","    return torch.tensor(self.examples[item])\n","\n","\n","# Build the dataset and display the dimensions of the 1st batch for verification:\n","slogan_dataset = SloganDataset('/content/drive/MyDrive/SloganGenerator/dataset/slogans.csv', tokenizer)\n","\n","print(next(iter(slogan_dataset)).size())\n","print(len(slogan_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7XSTxJHYKLE"},"outputs":[],"source":["import math, random\n","\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# Create data indices for training and validation splits:\n","indices = list(range(len(slogan_dataset)))\n","\n","random.seed(42)\n","random.shuffle(indices)\n","\n","split = math.floor(0.1 * len(slogan_dataset))\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Build the PyTorch data loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\n","val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)"]},{"cell_type":"markdown","source":["#Training"],"metadata":{"id":"8nMUrdCm2Zkh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9t2RwWNgE4l"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","\n","\n","def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n","  print(device)\n","  for i in range(epochs):\n","\n","    print('\\n--- Starting epoch #{} ---'.format(i))\n","\n","    model.train()\n","\n","    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n","    losses = []\n","    nums = []\n","\n","    for xb in tqdm(train_dl, desc=\"Training\"):\n","      # Move the batch to the training device:\n","      inputs = xb.to(device)\n","      # Call the model with the token ids, segment ids, and the ground truth (labels)\n","      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n","      # Add the loss and batch size to the list:\n","      loss = outputs[0]\n","      losses.append(loss.item())\n","      nums.append(len(xb))\n","\n","      loss.backward()\n","\n","      optimizer.step()\n","      model.zero_grad()\n","\n","    # Compute the average cost over one epoch:\n","    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n","\n","\n","    # Now do the same thing for validation:\n","    model.eval()\n","    \n","    with torch.no_grad():\n","      losses = []\n","      nums = []\n","\n","      for xb in tqdm(val_dl, desc=\"Validation\"):\n","        inputs = xb.to(device)\n","        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n","        losses.append(outputs[0].item())\n","        nums.append(len(xb))\n","\n","    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n","\n","    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48189,"status":"ok","timestamp":1685722932288,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"bPvm_dkUz8I0","outputId":"53e8f0ca-2302-4a40-de12-6bc14363d29f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/SloganGenerator/venv/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["cuda\n","\n","--- Starting epoch #0 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 268/268 [00:22<00:00, 11.96it/s]\n","Validation: 100%|██████████| 15/15 [00:00<00:00, 18.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch #0 finished --- Training cost: 4.399984752430635 / Validation cost: 3.2413841335713363\n","\n","--- Starting epoch #1 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 268/268 [00:21<00:00, 12.55it/s]\n","Validation: 100%|██████████| 15/15 [00:00<00:00, 18.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch #1 finished --- Training cost: 2.7163225437969274 / Validation cost: 3.286650333083978\n"]}],"source":["from transformers import AdamW\n","import os\n","\n","# Move the model to the GPU:\n","device = torch.device('cuda')\n","model.to(device)\n","\n","# Fine-tune GPT2 for two epochs:\n","optimizer = AdamW(model.parameters())\n","fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)\n","model.save_pretrained(\"/content/drive/MyDrive/SloganGenerator/models/gpt2_slogans\", from_pt=True) "]},{"cell_type":"markdown","source":["#Generation"],"metadata":{"id":"ak_AkswZ2eMY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cS_1D0tZDGG"},"outputs":[],"source":["# Sampling functions with top k and top p from HuggingFace:\n","\n","import torch.nn.functional as F\n","from tqdm import trange\n","\n","\n","def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n","    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n","        Args:\n","            logits: logits distribution shape (batch size x vocabulary size)\n","            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n","            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n","                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n","        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n","    \"\"\"\n","    top_k = min(top_k, logits.size(-1))  # Safety check\n","    if top_k > 0:\n","        # Remove all tokens with a probability less than the last token of the top-k\n","        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n","        logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","        # Remove tokens with cumulative probability above the threshold\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        # Shift the indices to the right to keep also the first token above the threshold\n","        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","        sorted_indices_to_remove[..., 0] = 0\n","\n","        # scatter sorted tensors to original indexing\n","        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n","        logits[indices_to_remove] = filter_value\n","    return logits\n","\n","\n","# From HuggingFace, adapted to work with the context/slogan separation:\n","def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n","                    device='cpu'):\n","    context = torch.tensor(context, dtype=torch.long, device=device)\n","    context = context.unsqueeze(0).repeat(num_samples, 1)\n","    generated = context\n","\n","    with torch.no_grad():\n","        for _ in trange(length):\n","\n","            inputs = {'input_ids': generated}\n","            if segments_tokens != None:\n","              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n","\n","            outputs = model(**inputs) \n","            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n","\n","            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n","            for i in range(num_samples):\n","                for _ in set(generated[i].tolist()):\n","                    next_token_logits[i, _] /= repetition_penalty\n","                \n","            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n","            if temperature == 0: # greedy sampling:\n","                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n","            else:\n","                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n","            generated = torch.cat((generated, next_token), dim=1)\n","    return generated\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3157,"status":"ok","timestamp":1685722980479,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"A1EjyJgCaqYN","outputId":"e60684b0-21e8-403f-fec6-b5720a65f8d6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:02<00:00,  4.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","--- Generated Slogans ---\n","\n"," Innovation starts here!\n"," Today Advance ahead in technology.\n"," Olivetti. The wiener by all cylinders\n"," Olivetti.What do you expect.\n"," Olivetti. What kind of ink? Money\n"," The power to Inform America. Partner in Comput\n"," Quartz. More than oxide. People.\n"," John Proven Orlin\n"," More than one machine.\n"," Visionors experts. Human touch.\n"," Get It Profit by Prints\n","  For people living today.\n"," Olivetti. First, Power the Power to\n"," ShareThe Power Tools. Software.\n"," Check-in more of things.\n"," Designed by those who Know What You Want.\n"," Eye operated by ink robots.\n"," Olivetti. Where Ideas and Ideas meet.\n"," Learn now for life's life.\n"," Violent time. Serious inspiration. Tread.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","\n","context = \"Olivetti is an Italian manufacturer of computers, tablets, smartphones, printers and other such business products as calculators and fax machines.\"\n","\n","context_tkn = tokenizer.additional_special_tokens_ids[0]\n","slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n","pad_tkn = tokenizer.pad_token_id\n","\n","input_ids = [context_tkn] + tokenizer.encode(context)\n","\n","segments = [slogan_tkn] * 64\n","segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n","\n","input_ids += [slogan_tkn]\n","\n","# Move the model back to the CPU for inference:\n","model.to(torch.device('cpu'))\n","\n","# Generate 20 samples of max length 20\n","generated = sample_sequence(model, length=10, context=input_ids, segments_tokens=segments, num_samples=20)\n","\n","print('\\n\\n--- Generated Slogans ---\\n')\n","\n","for g in generated:\n","  slogan = tokenizer.decode(g.squeeze().tolist())\n","  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\n","  print(slogan)  "]},{"cell_type":"markdown","source":["#Using merged dataset"],"metadata":{"id":"N4dKL6xX2p1b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ut_UVsgYpSI"},"outputs":[],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":739,"status":"ok","timestamp":1685722992726,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"0OxtBh2bY_25","outputId":"56aa57f7-a7f9-4c9a-afe1-973a87d4f6e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<context>', '<slogan>']}\n"]}],"source":["# Declare special tokens for padding and separating the context from the slogan:\n","SPECIAL_TOKENS_DICT = {\n","    'pad_token': '<pad>',\n","    'additional_special_tokens': ['<context>', '<slogan>'],\n","}\n","\n","# Add these special tokens to the vocabulary and resize model's embeddings:\n","tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Show the full list of special tokens:\n","print(tokenizer.special_tokens_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3994,"status":"ok","timestamp":1685722997891,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"sYyNz0ETBGv3","outputId":"9a6c687b-607c-4981-e696-fe4ac645e847"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 64])\n","11903\n"]}],"source":["# Build the dataset and display the dimensions of the 1st batch for verification:\n","merged_dataset = SloganDataset('/content/drive/My Drive/SloganGenerator/dataset/merged.csv', tokenizer)\n","print(next(iter(merged_dataset)).size())\n","print(len(merged_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MyAWljTBd00"},"outputs":[],"source":["# Create data indices for training and validation splits:\n","indices = list(range(len(merged_dataset)))\n","\n","random.seed(42)\n","random.shuffle(indices)\n","\n","split = math.floor(0.1 * len(merged_dataset))\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Build the PyTorch data loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","val_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = DataLoader(merged_dataset, batch_size=32, sampler=train_sampler)\n","val_loader = DataLoader(merged_dataset, batch_size=64, sampler=val_sampler)\n","# Note: we can double the batch size for validation since no backprogation is involved (thus it will fit on GPU's memory)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56620,"status":"ok","timestamp":1685723057710,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"BXNUIb29Bhh9","outputId":"df68548f-c0d0-4a1a-f7e4-188954b06161"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","\n","--- Starting epoch #0 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 335/335 [00:26<00:00, 12.52it/s]\n","Validation: 100%|██████████| 19/19 [00:00<00:00, 19.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch #0 finished --- Training cost: 4.186643159301588 / Validation cost: 3.316226381013373\n","\n","--- Starting epoch #1 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 335/335 [00:26<00:00, 12.54it/s]\n","Validation: 100%|██████████| 19/19 [00:00<00:00, 19.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Epoch #1 finished --- Training cost: 2.6350924810040826 / Validation cost: 3.405537839296485\n"]}],"source":["# Move the model to the GPU:\n","device = torch.device('cuda')\n","model.to(device)\n","\n","# Fine-tune GPT2 for two epochs:\n","optimizer = AdamW(model.parameters())\n","fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)\n","model.save_pretrained(\"/content/drive/MyDrive/SloganGenerator/models/gpt2_merged\", from_pt=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2546,"status":"ok","timestamp":1685723062376,"user":{"displayName":"Luca Caprioli","userId":"06935884608622521549"},"user_tz":-120},"id":"Tyz-2-0MBi40","outputId":"2100890a-4a91-4e5a-c7fb-9f6288caf7b5"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:02<00:00,  4.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","--- Generated Slogans ---\n","\n"," Olivetti. Do more.\n"," Olivetti. Just what you <context> Olive\n"," Olivettiis. Inspire.\n"," Olivetti. One of the good things.\n"," Oslivetti. Wunder pressure.\n"," Olivetti. The Smart Choice.\n"," Olivetti. Exceedingly superior.\n"," Olivetti. Put us first.\n"," Olivetti. A change in the noise\n"," Olivetti. As individual as you are\n"," Olivetti. Going beyond words is an opportunity\n"," Olivetti. Instruments tested gladly.\n"," Olivetti. The mother of the things we\n"," Olivetti. Designed for the mill.\n"," Olivetti. How ingenuity can be.\n"," Olivetti is an Italian company. It's\n"," Olivetti. The Right Instruments. For the\n"," Olivetti. Software that can borrow a little\n"," Olivetti. I am an Italian ITator\n"," Olivetti. (something for things you can\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["context = \"Olivetti is an Italian manufacturer of computers, tablets, smartphones, printers and other such business products as calculators and fax machines.\"\n","context_tkn = tokenizer.additional_special_tokens_ids[0]\n","slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n","\n","input_ids = [context_tkn] + tokenizer.encode(context)\n","\n","segments = [slogan_tkn] * 64\n","segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n","\n","input_ids += [slogan_tkn]\n","\n","# Move the model back to the CPU for inference:\n","model.to(torch.device('cpu'))\n","\n","# Generate 20 samples of max length 20\n","generated = sample_sequence(model, length=10, context=input_ids, segments_tokens=segments, num_samples=20)\n","\n","print('\\n\\n--- Generated Slogans ---\\n')\n","\n","for g in generated:\n","  slogan = tokenizer.decode(g.squeeze().tolist())\n","  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\n","  print(slogan)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}